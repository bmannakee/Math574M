\documentclass{article}
\usepackage[top=2in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{booktabs}
\pagestyle{fancyplain}
\begin{document}
\lhead{Math 574M\\ Homework 4}
\rhead{Brian Mannakee\\ \today}
\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\ovec}[1]{\mathbf{\Omega_{#1}}}
\newcommand{\normfront}[1]{(#1\pi)^{-\frac{1}{2}}}

\SweaveOpts{concordance=TRUE}

<<echo=FALSE,results=hide>>=
source('mannakee_homework4.r')


@

\begin{enumerate}
  \item[2.6] Consider points distributed in a p-dimensional unit ball. The median distance to the point closest to the center of the ball is given by the inverse CDF of the first order statistic of $R \sim Unif[0,1]$
  $$ median = F_{R_{(1)}}^{-1} $$
  Given that the CDF of $R_{(1)}$ is related to $P(R_i \ge r)$ by
  $$ 1 - F_{R_{(1)}}(r) = \prod_{i=1}^N P(R_i \ge r) = \prod_{i=1}^N 1-P(R_i \le r) $$
  $P(R_i \le r)$ for points uniformly distributed in the unit sphere is given by the ratio of the volume of a sphere with radius $r \in [0,1]$ to the volume of the unit sphere, and we previously derived a formula for the volume of a p-dimensional sphere $V_p(r)$, so
  $$ P(R_i \le r) = \frac{V_p(r)}{V_p(1)} = \frac{\frac{\pi^{p/2}r^p}{\Gamma(1 + \frac{p}{2})}}{\frac{\pi^{p/2}1^p}{\Gamma(1 + \frac{p}{2})}} = r^p$$
  Combining the above we have
  \begin{eqnarray*}
    F_{R_{(1)}}(r) &=& 1- (1-r^p)^N \\
    F_{R_{(1)}}^{-1}(x) &=& (1-x^{1/N})^{1/p} \\
    F_{R_{(1)}}^{-1}\left(\frac{1}{2}\right) &=& \left(1-\frac{1}{2}^{1/N}\right)^{1/p} //
  \end{eqnarray*}
  \item[3.3]
    \begin{enumerate}
      \item Let $c^Ty$ be an unbiased estimator of $a^T\beta$. Then
      \begin{eqnarray*}
        E(c^Ty) &=& a^T\beta \\
        c^TE(y) &=& a^T\beta \\
        cX^T\beta &=& a^T\beta
      \end{eqnarray*}
      Which requires that $c^TX = a^T$. Now we can write $Var(c^Ty) = Var(c^Ty - a^T\hat\beta + a^T\hat\beta)$ and simplify
      \begin{eqnarray*}
        Var(c^Ty - a^T\hat\beta + a^T\hat\beta) &=& Var(c^Ty - a^T\hat\beta) + 2Cov(c^Ty - a^T\hat\beta,a^T\hat\beta) + Var(a^T\hat\beta) \\        
      \end{eqnarray*}
      Which is greater than or equal to $Var(a^T\hat\beta)$ if $Cov(c^Ty - a^T\hat\beta,a^T\hat\beta) = 0$.
      \begin{eqnarray*}
        Cov(c^Ty - a^T\hat\beta,a^T\hat\beta) &=& Cov(c^Ty-a^T(X^TX)^{-1}X^Ty,a^T\hat\beta) \\
                                              &=& (c^T-a^T(X^TX)^{-1}X^T)Cov(y)(a^T(X^TX)^{-1}X^T)^T \\
                                              &=& \sigma^2\vec{I}(c^T-a^T(X^TX)^{-1}X^T)(X(X^TX)^{-1}a) \\
                                              &=& \sigma^2\vec{I}(c^TX(X^TX)^{-1}a - a^T(X^TX)^{-1}X^TX(X^TX)^{-1}a) \\
                                              &=& \sigma^2\vec{I}(c^TX(X^TX)^{-1}a - a^T(X^TX)^{-1}a) \\
                                              &=& \sigma^2\vec{I}(a^T(X^TX)^{-1}a - a^T(X^TX)^{-1}a) \text{ substituting from above} \\
                                              &=& 0
      \end{eqnarray*}
      So $Var(c^Ty) \ge Var(a^T\hat\beta)$ for any unbiased estimator $c^Ty$ of $a^T\beta$, where $a^T\hat\beta$ is the OLS estimator of $a^T\beta$. 
    \end{enumerate}
    \item[3.7]
      \begin{eqnarray*}
        \pi(\vec{\beta}|\vec{y}) &=& \frac{\prod_{i=1}^N\prod_{j=1}^p f_y(y_i|\beta_j)\prod_{j=1}^p\pi(\beta_j)}{m(y)} \\
                                 &\propto& exp\left[-\frac{1}{2}\frac{\sum_{i=1}^N(y_i - \beta_0 - \sum_{j=1}^px_{ij}^T\beta_j)^2}{\sigma^2}\right]
                                        + exp\left[-\frac{1}{2}\frac{\sum_{j=1}^p\beta_j^2}{\tau^2}\right] \\
                                 &\propto& exp\left[-\frac{1}{2}\frac{\sum_{i=1}^N(y_i - \beta_0 - \sum_{j=1}^px_{ij}^T\beta_i)^2}{\sigma^2} 
                                        + -\frac{1}{2}\frac{\sum_{j=1}^p\beta_i^2}{\tau^2}\right] \\
   log(\pi(\vec{\beta}|\vec{y})) &\propto& -\frac{1}{2\sigma^2\tau^2}\tau^2\sum_{i=1}^{N}(y_i - \beta_0 - \sum_{j=1}^px_{ij}^T\beta_i)^2 + \sigma^2\sum_{j=1}^p\beta_i^2 \\
  -log(\pi(\vec{\beta}|\vec{y})) &\propto& \sum_{i=1}^{N}(y_i - \beta_0 - \sum_{j=1}^px_{ij}^T\beta_i)^2 + \frac{\sigma^2}{\tau^2}\sum_{j=1}^p\beta_i^2 //\\
      \end{eqnarray*}
    \item
      \begin{enumerate}
        \item For the best subset estimator, we can see in the picture that what we are doing is setting a minimum absolute threshhold for values of $\hat{\beta_j}$ such that for any estimator, if it's absolute value is below the value of the estimator at which we set the threshhold $\hat{\beta_{(M)}}$ we set it to zero.\\ Mathematically $\hat{\beta_j} = \hat{\beta_j}\cdot\vec{I}(|\hat{\beta_j}| \ge |\hat{\beta_{(M)}}|)$
        \item We can write $\vec{\hat{\beta}^{ridge}} = \vec{(X^TX + \lambda I)^{-1}X^Ty}$ as in equation 3.44 in the book. In the orthogonal case $\vec{X^TX} = \vec{I}$ and it follows that $\vec{\hat{\beta^{ls}}} =  \vec{X^Ty}$. We thus have $\vec{\hat{\beta}^{ridge}} = \vec{\hat{\beta}^{ls}}(\vec{I} + \lambda\vec{I})^{-1}$, and for any j $\hat{\beta}_j^{ridge} = \frac{\hat{\beta}_j^{ls}}{1 + \lambda}$
          \item For the Lasso we need to find a solution for $min_{\beta_j} F(\beta_j)$ where $F(\beta_j) = (\beta_j - \tilde{\beta_j})^2 + \lambda|\beta_j|$. Because we can't take the derivative of the absolute value function we break $F$ into two functions such that 
        $$
          F(\beta_j) = \left\{ \begin{array}{lcl}F_1(\beta_j) =  (\beta_j - \tilde{\beta_j})^2 + \lambda \beta_j & & \beta_j \ge 0\\
                                          F_2(\beta_j) =  (\beta_j - \tilde{\beta_j})^2 - \lambda \beta_j & & \beta_j < 0
                        \end{array}
                        \right.
        $$
        Mulitplying these out and completing the square we get 
        $$
        F(\beta_j) = \left\{ \begin{array}{lcl}F_1(\beta_j) =  [\beta_j - (\tilde{\beta_j} - \frac{\lambda}{2})]^2 + \tilde{\beta}_j^2 - (\grave{\beta} -\frac{\lambda}{2}) & & \beta_j \ge 0\\
                                          F_2(\beta_j) =  [\beta_j - (\tilde{\beta_j} + \frac{\lambda}{2})]^2 + \tilde{\beta}_j^2 - (\grave{\beta} +\frac{\lambda}{2}) & & \beta_j < 0
                        \end{array}
                        \right.
        $$
        Taking derivatives and setting equal to zero we get
        $$
        min_{\beta_j}F(\beta_j) = \left\{ \begin{array}{l} min_{\beta_j}F_1(\beta_j) = \tilde{\beta}_j - \frac{\lambda}{2}\\
                                          min_{\beta_j}F_2(\beta_j) = \tilde{\beta}_j + \frac{\lambda}{2}
                        \end{array}
                        \right.
        $$
        Now we have three scenarios to consider. 
          \begin{enumerate}
            \item If $\tilde{\beta}_j < \frac{\lambda}{2}$ both functions are minimized at a value greater than zero, but since $F_2$ only takes values less than zero it has a min at zero. So we have $F_1(\tilde{\beta}_j - \frac{\lambda}{2}) = \tilde{\beta}_j^2 - (\tilde{\beta}_j - \frac{\lambda}{2})^2 < F_2(0) = \tilde{\beta}_j^2$ so $\hat{\beta}_j^{lasso} = \tilde{\beta}_j - \frac{\lambda}{2}$
            \item if $-\frac{\lambda}{2} < \tilde{\beta}_j < \frac{\lambda}{2}$ $F_1$ is at a minimum to the left of the origin and $F_2$ is at a minimum to the right, so in their ranges each has a minimum at $\hat{\beta}_j = 0$. The way we set up the problem, using $F_1$ when $\beta = 0$, we have $\hat{\beta}_j^{lasso} = \tilde{\beta}_j - \frac{\lambda}{2}$
            \item if $\tilde{\beta}_j > \frac{\lambda}{2}$ then both functions have a minimum to the left of the origin and by the opposite of the reasoning in (i) we have $\hat{\beta}_j^{lasso} = \tilde{\beta}_j + \frac{\lambda}{2}$
          \end{enumerate}
          So these are the solutions to the lasso. We can combine them to get Sign$(\hat{\beta}_j)(|\hat{\beta}_j| - \lambda)_+$
      \end{enumerate}
      \item Performing prediction using various variable selection techniques we get the following results. Code is in the included file mannakee\_homework4.r. The lasso appears to perform best from both a test error and error variance standpoint.
      
<<echo=FALSE,results=tex>>=
print(prob5.table)
@
\end{enumerate}
















\end{document}







